import torch
import time
import os
from pyannote.audio import Model, Inference
import speech_recognition as sr
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class WhisperSTT:
    def __init__(self):
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ cuda, ì—†ìœ¼ë©´ cpu
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")
        
        # PyAnnote embedding ëª¨ë¸ ë¡œë“œ
        try:
            self.embeddingmodel = Model.from_pretrained(
                checkpoint="pyannote/embedding", 
                cache_dir="models/pyannote",
                use_auth_token=os.getenv("HF_API_KEY")  # Hugging Face API í‚¤
            )
            
            # Inference ê°ì²´ ìƒì„± (í™”ì ì„ë² ë”© ì¶”ì¶œìš©)
            self.inference = Inference(
                self.embeddingmodel, 
                window="whole", 
                device=self.device
            )
            
            # ë©”ì¸ í™”ì ì„ë² ë”© (ê¸°ì¤€ì´ ë˜ëŠ” í™”ì)
            self.main_speaker_embedding = self.inference("austin.wav")
            print("âœ… PyAnnote embedding model loaded successfully")
            
        except Exception as e:
            print(f"âš ï¸ PyAnnote model loading failed: {e}")
            self.embeddingmodel = None
            self.inference = None
            self.main_speaker_embedding = None
        
        # Speech Recognition ì´ˆê¸°í™”
        self.recognizer = sr.Recognizer()
        
    def callback(self, recognizer, audio):
        """ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì½œë°± í•¨ìˆ˜"""
        try:
            print("ğŸ¤ Processing audio data...")
            
            # Google Speech Recognition ì‚¬ìš©
            # ì‹¤ì œ ì‚¬ìš©ì‹œì—ëŠ” Google API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤
            # í…ŒìŠ¤íŠ¸ ëª©ì ìœ¼ë¡œëŠ” ê¸°ë³¸ API í‚¤ ì‚¬ìš©
            
            # ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ WAV íŒŒì¼ë¡œ ì €ì¥
            transcribed_file = "transcribed-audio.wav"
            with open(transcribed_file, "wb") as f:
                f.write(audio.get_wav_data())
            
            # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • ì‹œì‘
            start = time.time()
            
            # Google Speech Recognitionìœ¼ë¡œ ìŒì„± ì¸ì‹
            output = recognizer.recognize_google(audio, language="ko-KR")
            
            # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
            end = time.time()
            
            print(f"ğŸ§ Listen Offline Transcription in {end - start:.2f} seconds: {output}")
            
            # í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì†Œë¬¸ì ë³€í™˜)
            clean_output = self.clean_text(output)
            
            # í™”ì ì„ë² ë”© ë¶„ì„ (PyAnnote ëª¨ë¸ì´ ë¡œë“œëœ ê²½ìš°)
            if self.inference and self.main_speaker_embedding is not None:
                self.analyze_speaker(transcribed_file)
            
            return clean_output
            
        except sr.UnknownValueError:
            print("âŒ Google Speech Recognition could not understand audio")
            return None
        except sr.RequestError as e:
            print(f"âŒ Could not request results from Google Speech Recognition service; {e}")
            return None
        except Exception as e:
            print(f"âŒ Error in callback: {e}")
            return None
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬"""
        if not text:
            return ""
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì •ê·œí™”
        clean_output = str(text).replace(".", "").replace(",", "").replace("?", "").replace("!", "").lower()
        print(f"ğŸ§¹ Cleaned text: {clean_output}")
        
        return clean_output
    
    def analyze_speaker(self, audio_file):
        """í™”ì ë¶„ì„ (PyAnnote embedding ì‚¬ìš©)"""
        try:
            # í˜„ì¬ ì˜¤ë””ì˜¤ì˜ í™”ì ì„ë² ë”© ì¶”ì¶œ
            current_embedding = self.inference(audio_file)
            
            # ë©”ì¸ í™”ìì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            similarity = torch.cosine_similarity(
                self.main_speaker_embedding.unsqueeze(0), 
                current_embedding.unsqueeze(0)
            )
            
            print(f"ğŸ­ Speaker similarity: {similarity.item():.3f}")
            
            # ìœ ì‚¬ë„ê°€ 0.8 ì´ìƒì´ë©´ ê°™ì€ í™”ìë¡œ íŒë‹¨
            if similarity.item() > 0.8:
                print("âœ… Same speaker detected")
            else:
                print("ğŸ†• Different speaker detected")
                
        except Exception as e:
            print(f"âš ï¸ Speaker analysis failed: {e}")
    
    def start_listening(self):
        """ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œì‘"""
        print("ğŸ¤ Starting real-time speech recognition...")
        print("ğŸ’¡ Speak into your microphone. Press Ctrl+C to stop.")
        
        with sr.Microphone() as source:
            # í™˜ê²½ ì†ŒìŒ ì¡°ì •
            print("ğŸ”§ Adjusting for ambient noise... Please wait.")
            self.recognizer.adjust_for_ambient_noise(source, duration=2)
            print("âœ… Ready! Start speaking...")
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œì‘
        stop_listening = self.recognizer.listen_in_background(
            sr.Microphone(), 
            self.callback,
            phrase_time_limit=5  # 5ì´ˆë§ˆë‹¤ ì²˜ë¦¬
        )
        
        try:
            while True:
                time.sleep(0.1)  # CPU ì‚¬ìš©ë¥  ë‚®ì¶”ê¸°
        except KeyboardInterrupt:
            print("\nğŸ›‘ Stopping speech recognition...")
            stop_listening(wait_for_stop=False)
    
    def transcribe_file(self, audio_file_path):
        """íŒŒì¼ì—ì„œ ìŒì„± ì¸ì‹"""
        try:
            with sr.AudioFile(audio_file_path) as source:
                audio = self.recognizer.record(source)
            
            print(f"ğŸµ Transcribing file: {audio_file_path}")
            start = time.time()
            
            # Google Speech Recognition
            result = self.recognizer.recognize_google(audio, language="ko-KR")
            
            end = time.time()
            print(f"ğŸ“ Transcription completed in {end - start:.2f} seconds")
            print(f"ğŸ“„ Result: {result}")
            
            # í™”ì ë¶„ì„
            if self.inference:
                self.analyze_speaker(audio_file_path)
            
            return self.clean_text(result)
            
        except Exception as e:
            print(f"âŒ File transcription failed: {e}")
            return None

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # WhisperSTT ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    stt = WhisperSTT()
    
    print("=" * 50)
    print("ğŸ¤ WhisperSTT + PyAnnote ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ")
    print("=" * 50)
    print("1. ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹")
    print("2. ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„")
    print("=" * 50)
    
    choice = input("ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2): ")
    
    if choice == "1":
        # ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œì‘
        stt.start_listening()
    
    elif choice == "2":
        # íŒŒì¼ ê²½ë¡œ ì…ë ¥
        file_path = input("ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
        if os.path.exists(file_path):
            result = stt.transcribe_file(file_path)
            if result:
                print(f"\nâœ… ìµœì¢… ê²°ê³¼: {result}")
        else:
            print("âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

"""
í•„ìš”í•œ ì„¤ì •:

1. .env íŒŒì¼ì— ì¶”ê°€:
HF_API_KEY=your_huggingface_api_token

2. ì˜ì¡´ì„± ì„¤ì¹˜:
pip install torch
pip install pyannote.audio
pip install speechrecognition
pip install pyaudio

3. Hugging Face í† í° ìƒì„±:
- https://huggingface.co/settings/tokens ì—ì„œ í† í° ìƒì„±
- pyannote ëª¨ë¸ ì‚¬ìš© ê¶Œí•œ ìŠ¹ì¸ í•„ìš”

4. ê¸°ì¤€ í™”ì ì˜¤ë””ì˜¤ íŒŒì¼:
- austin.wav íŒŒì¼ì„ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ë°°ì¹˜
- ë˜ëŠ” ì½”ë“œì—ì„œ ë‹¤ë¥¸ íŒŒì¼ëª…ìœ¼ë¡œ ìˆ˜ì •
"""